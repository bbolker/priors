---
title: "notes on priors"
author: "Ben Bolker, Jen Freeman and ??"
---

## Terminology

- uninformative, weak, vague, flat, diffuse, *improper*
  - in a general sense, the data provides more information than the prior
  - *improper*- integral of prior distribution is infinite
- informative: contains sufficient subject matter knowledge which introduces constraints. Has the computational benefit of a smaller parameter space to explore [@wesner_choosing_2021].
- neutral (e.g. symmetric and centered at zero for fixed effects, or equivalently centered at the population mean)
- non-neutral (e.g. based on prior studies)

## normal or power-exponential priors

- normal construction: on an appropriate scale (e.g. natural, log, or logit), determine the lower/upper bounds of a 'reasonable' range and the tail probability, i.e. the overall probability of lying *outside* the reasonable range (e.g. 5%, 1%, 0.1% ...). This then determines the mean (halfway between the bounds) and standard deviation of a Gaussian prior.
- power-exponential construction: as above, but further defining the fraction of the power in the middle 50% of the central region determines the shape parameter of a power-exponential

## know your parameters

In order to set sensible informative (but neutral) priors, you need to know the meaning of your priors.  It is often easiest

The amount of information to include in the prior depends on the parameter type [@gelman_bayesian_2020]. Central quantity parameters permit weaker priors than scale parameters, followed by shape parameters [@gelman_bayesian_2020].



### Example:


## Cromwell's rule
  - > think it possible that you may be mistaken
- don't set a prior to zero for any parameter value that is theoretically/physically possible
- more generally, conceptual/computational problems with uniform distributions
    - discontinuous change in probability across the boundary
	- if bounds are active, posterior prob will pile up at the edge [@carpenter_computational_2017]


## Uninformed/weak is a bad idea

Already well covered elsewhere.

* uninformative is not as uninformative as you think: scales matter
* for any kind of bounded domain, large variances will make probability pile up near the edge(s) of the domain (inverse-gamma example from above)
* particular bad examples:
    * wide priors on a logit scale imply that probability is close to zero or 1 (Simpson?)
	* wide priors for regression parameters imply that $R^2$ is large (Vehtari?)
	
## Variance parameters

Variance parameters are a special category, one which Bayesian researchers have spent a great deal of time and effort on. In the Old Days the suggestion was to use inverse-Gamma (because this is the conjugate prior for a Gaussian variance parameter); people would use a neutral mean (e.g. of 1 assuming the response is sensible scaled) and make the shape parameter very small (e.g. inverse-Gamma(shape=0.0001, rate=0.0001), but this leads to a spike near zero [@gelman_prior_2006]


## Prior predictive simulations

The goal is to choose appropriate priors after performing checks on whether they produce reasonable outcomes without using observed data. The method is to repeatedly simulate data from your model by drawing parameters from your proposed prior distributions. An expert in the subject matter would assess whether the simulated data could pass as real data, and if not then the priors need to be changed. There are obvious benefits to graphing the simulated data when assessing the priors [@wesner_choosing_2021].

- Parameter expanded (half-Cauchy / t) [@gelman_data_2006; @hadfield_mcmc_2010]
- multivariate:
  - param-exp wishart
  - LKJ
- Separable correlations and std devs
- prior predictive simulations
- improper vs weak vs regularizing / shrinking / neutral vs non neutral (McCarthy)
- Reparametrizations (R_0 and r) or aux priors (init conditions)
- Avoid bad tails (flatness)
- advantage of joint priors over individual on large numbers of parameters [@gelman_bayesian_2020]
- strategies for choosing prior (matching centre, matching interval, "visualizing probability mass allocation") [@sarma_prior_2020]

